{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8116cce4-094c-4167-9b3e-5a26b4598c2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\alin_\\anaconda3\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\alin_\\anaconda3\\lib\\site-packages (0.3.30)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\alin_\\anaconda3\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: chromadb in c:\\users\\alin_\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith in c:\\users\\alin_\\anaconda3\\lib\\site-packages (0.4.32)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain) (0.3.78)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.35.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (1.75.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langsmith) (24.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langsmith) (0.23.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.41.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alin_\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading pymupdf-1.26.4-cp39-abi3-win_amd64.whl (18.7 MB)\n",
      "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 10.7/18.7 MB 61.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.7/18.7 MB 51.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf, langchain-huggingface\n",
      "Successfully installed langchain-huggingface-0.3.1 pymupdf-1.26.4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple PDF RAG System - Complete Setup\n",
    "Using FREE: Ollama LLM + HuggingFace Embeddings\n",
    "\n",
    "Prerequisites:\n",
    "- Install Ollama: https://ollama.ai\n",
    "- Run: ollama pull llama3.2\n",
    "- Run: ollama code \n",
    "- not sure if we can call llama3.2 and olama code depending on the question\n",
    "- !pip install langchain langchain-community langchain-huggingface sentence-transformers chromadb pymupdf langsmith\n",
    "- \n",
    "\"\"\"\n",
    "# !pip install langchain langchain-community langchain-huggingface sentence-transformers chromadb pymupdf langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b95fd17-e715-41ca-8b00-855031fd2caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LangSmith tracing enabled\n"
     ]
    }
   ],
   "source": [
    "# Setup Environment Variables\n",
    "\n",
    "import os\n",
    "\n",
    "# LangSmith Configuration\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'YOUR API KEY'\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'rag-project'  # Name your project for organization\n",
    "\n",
    "print(\"‚úì LangSmith tracing enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ca59e84-1a2c-4271-9c2f-13a4fabaa7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a479e6a6-20a7-4152-8c1b-5ae87e881dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "PARSED_JSON_FOLDER = \"./parsed_json\"\n",
    "PDF_FOLDER = \"./raw_data\"\n",
    "CHROMA_DIR = \"./chroma_db\"\n",
    "DEFAULT_COURSE = \"RAG ALIN\"\n",
    "\n",
    "# Embedding model\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Ollama model\n",
    "OLLAMA_MODEL = \"llama3.2\"  # \"codellama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fc4046-2384-4b9b-bcb3-be799f2b2427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize Embeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n",
    "print(f\"Embeddings model loaded: {EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908ca64c-3d6c-4103-9ea3-2283764ebb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Processing 89 JSON files (code)...\n",
      "\n",
      "‚úì 0010_Input.json: 5 chunks\n",
      "‚úì 0010_Loop_Example.json: 29 chunks\n",
      "‚úì 0020_If_Statement_ComparisonOperators.json: 15 chunks\n",
      "‚úì 0050_loops_nested.json: 5 chunks\n",
      "‚úì 0060_Relational_Operators.json: 7 chunks\n",
      "‚úì 0_Showcase_Shiny.json: 1 chunks\n",
      "‚úì 1_Shiny_UI_Editor.json: 1 chunks\n",
      "‚úì 2_1_mini_example_TextInput.json: 1 chunks\n",
      "‚úì 2_2_mini_example_NumericInput.json: 1 chunks\n",
      "‚úì 2_3_advanced_example_MultipleInputOutput.json: 6 chunks\n",
      "‚úì 3_Getting_Started.json: 1 chunks\n",
      "‚úì 4_R_PullMechanism.json: 1 chunks\n",
      "‚úì 5_Error_Example_ReactiveContext.json: 1 chunks\n",
      "‚úì 6_Error_Example_OutputName.json: 2 chunks\n",
      "‚úì 7_Error_Example_ReactiveExpressionsBrackets.json: 3 chunks\n",
      "‚úì Bad_Example_01.json: 2 chunks\n",
      "‚úì Bad_Example_Autoformatting_01.json: 2 chunks\n",
      "‚úì Basic_Rmarkdown.json: 1 chunks\n",
      "‚úì challenge_plot.json: 2 chunks\n",
      "‚úì CleanUp_iris.json: 10 chunks\n",
      "‚úì comp_lambda_i.json: 2 chunks\n",
      "‚úì comp_lambda_ii.json: 2 chunks\n",
      "‚úì comp_lambda_ii_solution.json: 1 chunks\n",
      "‚úì comp_lambda_i_solution.json: 2 chunks\n",
      "‚úì continent_example.json: 3 chunks\n",
      "‚úì control_structures.json: 2 chunks\n",
      "‚úì control_structures_solution.json: 4 chunks\n",
      "‚úì democode.json: 3 chunks\n",
      "‚úì democode_data_manipulation.json: 10 chunks\n",
      "‚úì democode_ggplot2.json: 8 chunks\n",
      "‚úì democode_maps_v2.json: 10 chunks\n",
      "‚úì democode_regex.json: 6 chunks\n",
      "‚úì democode_R_programming.json: 5 chunks\n",
      "‚úì democode_unit_tests.json: 16 chunks\n",
      "‚úì example_code_fs25.json: 317 chunks\n",
      "‚úì Exercises_Loops_and_Conditions.json: 4 chunks\n",
      "‚úì Exercises_Loops_and_Conditions_Solution.json: 6 chunks\n",
      "‚úì exercise_animal_classes.json: 4 chunks\n",
      "‚úì exercise_animal_classes_inherit.json: 4 chunks\n",
      "‚úì exercise_animal_classes_inherit_solution.json: 9 chunks\n",
      "‚úì exercise_animal_classes_solution.json: 4 chunks\n",
      "‚úì Exercise_Conditions_01.json: 3 chunks\n",
      "‚úì Exercise_Conditions_01_Solutions.json: 3 chunks\n",
      "‚úì Exercise_Conditions_02.json: 2 chunks\n",
      "‚úì Exercise_Conditions_02_Solutions.json: 1 chunks\n",
      "‚úì exercise_file_access_collection.json: 1 chunks\n",
      "‚úì exercise_file_access_solutions.json: 1 chunks\n",
      "‚úì exercise_formatting_collection.json: 1 chunks\n",
      "‚úì exercise_formatting_collection_solution.json: 2 chunks\n",
      "‚úì exercise_functions_collection.json: 4 chunks\n",
      "‚úì exercise_functions_solutions.json: 3 chunks\n",
      "‚úì exercise_kwargs.json: 1 chunks\n",
      "‚úì exercise_kwargs_solution.json: 3 chunks\n",
      "‚úì function_packing.json: 2 chunks\n",
      "‚úì function_packing_solution.json: 4 chunks\n",
      "‚úì GLMs_Lab.json: 19 chunks\n",
      "‚úì GLMs_Lab_py.json: 15 chunks\n",
      "‚úì interactive_example.json: 1 chunks\n",
      "‚úì intro_drop_columns_rows.json: 2 chunks\n",
      "‚úì intro_insert_and_update_columns_rows.json: 3 chunks\n",
      "‚úì intro_read_files.json: 4 chunks\n",
      "‚úì intro_slicing.json: 2 chunks\n",
      "‚úì kaggle_scrape.json: 1 chunks\n",
      "‚úì LM_Cats_Lab.json: 12 chunks\n",
      "‚úì LM_Cats_Lab_py.json: 25 chunks\n",
      "‚úì LM_ResidualAnalysis_Lab.json: 5 chunks\n",
      "‚úì LM_ResidualTransformations_Lab.json: 6 chunks\n",
      "‚úì LM_Trees_Graphs_Lab.json: 4 chunks\n",
      "‚úì LM_Trees_NonLinearityLab.json: 13 chunks\n",
      "‚úì LM_Tree_Growth_Lab.json: 8 chunks\n",
      "‚úì LM_Tree_Growth_Lab_py.json: 8 chunks\n",
      "‚úì LM_Tree_Grphs_Lab_py.json: 4 chunks\n",
      "‚úì Minimal_Rmarkdown_Example.json: 2 chunks\n",
      "‚úì oop_basics.json: 3 chunks\n",
      "‚úì oop_basics_solution.json: 4 chunks\n",
      "‚úì readfile_loops_comprehension.json: 3 chunks\n",
      "‚úì readfile_loops_comprehension_solution.json: 3 chunks\n",
      "‚úì run_script.json: 1 chunks\n",
      "‚úì run_script_solution.json: 1 chunks\n",
      "‚úì server.json: 2 chunks\n",
      "‚úì short_expressions.json: 1 chunks\n",
      "‚úì short_expressions_solution.json: 2 chunks\n",
      "‚úì strings_comprehension_functions.json: 3 chunks\n",
      "‚úì strings_comprehension_functions_solution.json: 4 chunks\n",
      "‚úì string_operations.json: 1 chunks\n",
      "‚úì string_operations_solution.json: 1 chunks\n",
      "‚úì Transforming_coordinate_systems_CH.json: 2 chunks\n",
      "‚úì ui.json: 1 chunks\n",
      "‚úì yahoo.json: 2 chunks\n",
      "\n",
      "üìä Total code chunks: 717\n"
     ]
    }
   ],
   "source": [
    "#Process JSON Files (Code) - With LangSmith Tracing\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"process_json_files\")\n",
    "def process_json_files(json_folder: str, chroma_dir: str):\n",
    "    \"\"\"\n",
    "    Process JSON files (code) with LangSmith tracing\n",
    "    \"\"\"\n",
    "    json_path = Path(json_folder)\n",
    "    json_files = list(json_path.glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"\\nProcessing {len(json_files)} JSON files (code)...\\n\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # Code-aware text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\nclass \", \"\\n\\ndef \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            content = data.get(\"content\", \"\")\n",
    "            if not content:\n",
    "                print(f\"Skipping {json_file.name}: no content\")\n",
    "                continue\n",
    "            \n",
    "            # Create Document with metadata\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": data.get(\"name\", \"unknown\"),\n",
    "                    \"extension\": data.get(\"extension\", \"\"),\n",
    "                    \"course\": data.get(\"course\", DEFAULT_COURSE),\n",
    "                    \"file_type\": \"code\",\n",
    "                    \"original_size\": data.get(\"st_size\", 0),\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Split into chunks\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            print(f\"{json_file.name}: {len(chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file.name}: {e}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Run with tracing\n",
    "code_chunks = process_json_files(PARSED_JSON_FOLDER, CHROMA_DIR)\n",
    "print(f\"\\nTotal code chunks: {len(code_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "259ebb1d-92e2-4ccb-bcc4-286d113a9b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Processing 64 PDF files (including subfolders)...\n",
      "\n",
      "‚úì Problems_09.pdf (course: Bayesian and Classic Statistics): 21 chunks\n",
      "‚úì Slides_09_handout.pdf (course: Bayesian and Classic Statistics): 65 chunks\n",
      "‚úì Problems_01.pdf (course: Bayesian and Classic Statistics): 17 chunks\n",
      "‚úì Slides_handout_01.pdf (course: Bayesian and Classic Statistics): 106 chunks\n",
      "‚úì Problems_10.pdf (course: Bayesian and Classic Statistics): 18 chunks\n",
      "‚úì Slides_10_print.pdf (course: Bayesian and Classic Statistics): 32 chunks\n",
      "‚úì Problems_11.pdf (course: Bayesian and Classic Statistics): 15 chunks\n",
      "‚úì Slides_11_print.pdf (course: Bayesian and Classic Statistics): 35 chunks\n",
      "‚úì Problems_12.pdf (course: Bayesian and Classic Statistics): 27 chunks\n",
      "‚úì Slides_12_handout.pdf (course: Bayesian and Classic Statistics): 73 chunks\n",
      "‚úì Problems_13.pdf (course: Bayesian and Classic Statistics): 34 chunks\n",
      "‚úì Slides_13_handout.pdf (course: Bayesian and Classic Statistics): 16 chunks\n",
      "‚úì Problems_02.pdf (course: Bayesian and Classic Statistics): 17 chunks\n",
      "‚úì Slides_handout_02.pdf (course: Bayesian and Classic Statistics): 84 chunks\n",
      "‚úì Slides_print_02.pdf (course: Bayesian and Classic Statistics): 40 chunks\n",
      "‚úì Problem_03.pdf (course: Bayesian and Classic Statistics): 8 chunks\n",
      "‚úì Slides_03_handout.pdf (course: Bayesian and Classic Statistics): 50 chunks\n",
      "‚úì Problems_04.pdf (course: Bayesian and Classic Statistics): 12 chunks\n",
      "‚úì Slides_04_print.pdf (course: Bayesian and Classic Statistics): 29 chunks\n",
      "‚úì Problems_05.pdf (course: Bayesian and Classic Statistics): 5 chunks\n",
      "‚úì Slides_05_print.pdf (course: Bayesian and Classic Statistics): 23 chunks\n",
      "‚úì Problems_06.pdf (course: Bayesian and Classic Statistics): 46 chunks\n",
      "‚úì Slides_06_print.pdf (course: Bayesian and Classic Statistics): 32 chunks\n",
      "‚úì Problems_07.pdf (course: Bayesian and Classic Statistics): 21 chunks\n",
      "‚úì Problems_08.pdf (course: Bayesian and Classic Statistics): 24 chunks\n",
      "‚úì Slides_handout_08.pdf (course: Bayesian and Classic Statistics): 73 chunks\n",
      "‚úì SA combined exercises and solutions.pdf (course: Bayesian and Classic Statistics): 321 chunks\n",
      "‚úì main_pruefung_SA_HS18_eng.pdf (course: Bayesian and Classic Statistics): 15 chunks\n",
      "‚úì main_pruefung_SA_HS18_eng_sol.pdf (course: Bayesian and Classic Statistics): 29 chunks\n",
      "‚úì Testpruefung Loesung,pdf.pdf (course: Bayesian and Classic Statistics): 22 chunks\n",
      "‚úì Testpruefung.pdf (course: Bayesian and Classic Statistics): 14 chunks\n",
      "‚úì FS19_solution.pdf (course: Bayesian and Classic Statistics): 28 chunks\n",
      "‚úì SA_FS19_eng_test.pdf (course: Bayesian and Classic Statistics): 17 chunks\n",
      "‚úì Exercises_Solutions_1.pdf (course: Machine Learning 1): 15 chunks\n",
      "‚úì LM_Cats_Lab.pdf (course: Machine Learning 1): 37 chunks\n",
      "‚úì LM_Tree_Growth_Lab_py.pdf (course: Machine Learning 1): 26 chunks\n",
      "‚úì Slides_1a_ML1.pdf (course: Machine Learning 1): 19 chunks\n",
      "‚úì Slides_1b_ML1.pdf (course: Machine Learning 1): 6 chunks\n",
      "‚úì exercises_solutions_2.pdf (course: Machine Learning 1): 15 chunks\n",
      "‚úì LM_Trees_NonLinearityLab_py.pdf (course: Machine Learning 1): 26 chunks\n",
      "‚úì LM_Tree_Graphs_Lab_py.pdf (course: Machine Learning 1): 25 chunks\n",
      "‚úì Slides_2_ML1.pdf (course: Machine Learning 1): 10 chunks\n",
      "‚úì sw01.basics.pdf (course: Python for data science): 29 chunks\n",
      "‚úì sw10.ext.packages.pdf (course: Python for data science): 22 chunks\n",
      "‚úì sw11.short.fnc.pdf (course: Python for data science): 24 chunks\n",
      "‚úì sw13.exceptions.pdf (course: Python for data science): 22 chunks\n",
      "‚úì sw02.if.else.pdf (course: Python for data science): 26 chunks\n",
      "‚úì Exercises_Loops_and_Conditions.pdf (course: Python for data science): 4 chunks\n",
      "‚úì Exercises_Loops_and_Conditions_Solution.pdf (course: Python for data science): 5 chunks\n",
      "‚úì sw03.for.while.pdf (course: Python for data science): 25 chunks\n",
      "‚úì sw04.debugging.pdf (course: Python for data science): 16 chunks\n",
      "‚úì sw05.fnc.files.str.pdf (course: Python for data science): 19 chunks\n",
      "‚úì sw06.fnc_ii.recursion.pdf (course: Python for data science): 20 chunks\n",
      "‚úì sw07.oop.i.pdf (course: Python for data science): 21 chunks\n",
      "‚úì sw08.oop.ii.pdf (course: Python for data science): 24 chunks\n",
      "‚úì sw09.packaging.pdf (course: Python for data science): 27 chunks\n",
      "‚úì 01_Part_I_FS23.pdf (course: Python for data science): 9 chunks\n",
      "‚úì Python solution 2023.pdf (course: Python for data science): 21 chunks\n",
      "‚úì manus_python_exam_summary.pdf (course: Python for data science): 139 chunks\n",
      "‚úì short_expressions_lambda_guide.pdf (course: Python for data science): 42 chunks\n",
      "‚úì mockup.exam.part_1.pdf (course: Python for data science): 9 chunks\n",
      "‚úì mockup.exam.part_2.pdf (course: Python for data science): 4 chunks\n",
      "‚úì solutions.mockup.exam.part_1.pdf (course: Python for data science): 4 chunks\n",
      "‚úì python_mock_exam.pdf (course: Python for data science): 18 chunks\n",
      "\n",
      "üìä Total PDF chunks: 2078\n"
     ]
    }
   ],
   "source": [
    "# Process PDFs - With LangSmith Tracing\n",
    "\n",
    "@traceable(name=\"process_pdfs\")\n",
    "def process_pdfs(pdf_folder: str):\n",
    "    \"\"\"\n",
    "    Process PDFs with LangSmith tracing\n",
    "    \"\"\"\n",
    "    pdf_path = Path(pdf_folder)\n",
    "    pdf_files = list(pdf_path.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"\\nüìÑ Processing {len(pdf_files)} PDF files (including subfolders)...\\n\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # PDF text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            # Load PDF\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            pages = loader.load()\n",
    "            \n",
    "            # Extract course from folder structure\n",
    "            try:\n",
    "                relative_path = pdf_file.relative_to(pdf_path)\n",
    "                course = relative_path.parts[0] if len(relative_path.parts) > 1 else DEFAULT_COURSE\n",
    "            except:\n",
    "                course = DEFAULT_COURSE\n",
    "            \n",
    "            # Split into chunks\n",
    "            chunks = text_splitter.split_documents(pages)\n",
    "            \n",
    "            # Add metadata\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata[\"course\"] = course\n",
    "                chunk.metadata[\"file_type\"] = \"pdf\"\n",
    "                chunk.metadata[\"source\"] = pdf_file.stem\n",
    "            \n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            print(f\"‚úì {pdf_file.name} (course: {course}): {len(chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error processing {pdf_file.name}: {e}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Run with tracing\n",
    "pdf_chunks = process_pdfs(PDF_FOLDER)\n",
    "print(f\"\\nüìä Total PDF chunks: {len(pdf_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76bda61b-5005-4863-a4c8-da10a42da760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Total documents: 2795\n",
      "‚è≥ Adding to Chroma (this may take a minute)...\n",
      "‚úÖ Vector store created successfully!\n",
      "\n",
      "üìä Total chunks in Chroma: 2795\n"
     ]
    }
   ],
   "source": [
    "#Create Chroma Vector Store - With Tracing\n",
    "\n",
    "@traceable(name=\"create_vectorstore\")\n",
    "def create_vectorstore(all_documents: list, chroma_dir: str):\n",
    "    \"\"\"\n",
    "    Create Chroma vector store with LangSmith tracing\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì¶ Total documents: {len(all_documents)}\")\n",
    "    print(\"‚è≥ Adding to Chroma (this may take a minute)...\")\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=all_documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=chroma_dir\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Vector store created successfully!\")\n",
    "    return vectorstore\n",
    "\n",
    "# Combine all chunks\n",
    "all_documents = code_chunks + pdf_chunks\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = create_vectorstore(all_documents, CHROMA_DIR)\n",
    "\n",
    "# Verify\n",
    "collection = vectorstore._collection\n",
    "print(f\"\\nüìä Total chunks in Chroma: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e17f92eb-8518-4386-ba05-cc34cced5483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG chain created with Ollama\n",
      "‚úì Model: llama3.2\n",
      "‚úì LangSmith will trace all queries automatically!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alin_\\AppData\\Local\\Temp\\ipykernel_19728\\1935592373.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "# Setup RAG Chain with Ollama - With Full Tracing\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=OLLAMA_MODEL,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Create custom prompt\n",
    "prompt_template = \"\"\"You are a helpful assistant answering questions based on course materials and code.\n",
    "Use the following context to answer the question. If you don't know the answer based on the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG chain created with Ollama\")\n",
    "print(f\"‚úì Model: {OLLAMA_MODEL}\")\n",
    "print(\"‚úì LangSmith will trace all queries automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6cede90-2a16-4343-8cea-88bf7fd7db87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Question: How do I create a pandas dataframe?\n",
      "\n",
      "üí° Answer:\n",
      "To create a pandas DataFrame, you can use the `pd.DataFrame()` function or the `pd.read_csv()`, `pd.read_json()`, etc. functions depending on how your data is formatted.\n",
      "\n",
      "Here are some examples:\n",
      "\n",
      "- From a dictionary:\n",
      "```python\n",
      "data = {'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
      "        'Age': [28, 24, 35, 32],\n",
      "        'City': ['New York', 'Paris', 'Tokyo', 'Sydney']}\n",
      "df = pd.DataFrame(data)\n",
      "```\n",
      "\n",
      "- From a CSV file:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv\"\n",
      "tips = pd.read_csv(url)\n",
      "print(tips.head())\n",
      "```\n",
      "\n",
      "- From an Excel file:\n",
      "```python\n",
      "df = pd.read_excel('data.xlsx')\n",
      "```\n",
      "\n",
      "- From a JSON file:\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "url = \"https://next.json-generator.com/api/json/get/4yIPxvyGt\"\n",
      "data = pd.read_json(url)\n",
      "print(data.head())\n",
      "```\n",
      "\n",
      "Note that the `pd.DataFrame()` function takes in a dictionary, list of lists or a numpy array.\n",
      "\n",
      "üìö Sources used:\n",
      "\n",
      "1. sw10.ext.packages\n",
      "   Type: pdf\n",
      "   Course: Python for data science\n",
      "   Page: 7\n",
      "   Preview: The Pandas library is a powerful and widely-used tool in Python for data manipulation and analysis. It provides a variety of\n",
      "features and functionalit...\n",
      "\n",
      "2. intro_insert_and_update_columns_rows\n",
      "   Type: code\n",
      "   Course: CIP\n",
      "   Preview: # line to insert\n",
      "insert_line1 = [11, 1200 , 15, \"Switzerland\", 30, \"Friday\"]\n",
      "\n",
      "# create a new DataFrame with the same column_names as fd1 ...\n",
      "df_to_add...\n",
      "\n",
      "3. sw10.ext.packages\n",
      "   Type: pdf\n",
      "   Course: Python for data science\n",
      "   Page: 6\n",
      "   Preview: For data handling, data scientists often work with data frames. These allow data scientists to bring data in a structured order\n",
      "that facilitates data ...\n",
      "\n",
      "4. intro_drop_columns_rows\n",
      "   Type: code\n",
      "   Course: CIP\n",
      "   Preview: df23 = __FILL_HERE__\n",
      "index_5 = __FILL_HERE__\n",
      "\n",
      "# print df23\n",
      "\n",
      "#################\n",
      "# Example (simple)\n",
      "#################\n",
      "\n",
      "df1 = pandas.read_csv(\"traffic.csv...\n",
      "\n",
      "5. intro_read_files\n",
      "   Type: code\n",
      "   Course: CIP\n",
      "   Preview: df12 = pandas.read_json(__FILL_HERE__)\n",
      "\n",
      "# Recommend :try out yourself: \n",
      "\n",
      "# https://next.json-generator.com/api/json/get/4yIPxvyGt...\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG System - Fully Traced!\n",
    "\n",
    "def ask_question(question: str):\n",
    "    \"\"\"\n",
    "    Ask a question to the RAG system\n",
    "    All steps are automatically traced in LangSmith!\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Question: {question}\\n\")\n",
    "    \n",
    "    # Query (automatically traced by LangSmith!)\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    # Display answer\n",
    "    print(f\"üí° Answer:\\n{result['result']}\\n\")\n",
    "    \n",
    "    # Display sources\n",
    "    print(\"üìö Sources used:\")\n",
    "    for i, doc in enumerate(result['source_documents'], 1):\n",
    "        print(f\"\\n{i}. {doc.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"   Type: {doc.metadata.get('file_type', 'unknown')}\")\n",
    "        print(f\"   Course: {doc.metadata.get('course', 'unknown')}\")\n",
    "        if doc.metadata.get('file_type') == 'pdf':\n",
    "            print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "        print(f\"   Preview: {doc.page_content[:150]}...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test it!\n",
    "result = ask_question(\"How do I create a pandas dataframe?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
